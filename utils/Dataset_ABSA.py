#!/usr/bin/python3# -*- coding: utf-8 -*-# @Time    : 2019-09-03 16:08# @Author  : apollo2mars# @File    : Dataset_ABSA.py# @Contact : apollo2mars@gmail.com# @Desc    :class Dataset_ABSA(Dataset):    def __init__(self, fname, tokenizer):        fin = open(fname, 'r', encoding='utf-8', newline='\n', errors='ignore')        lines = fin.readlines()        fin.close()        all_data = []        for i in range(0, len(lines), 4):            text_left, _, text_right = [s.lower().strip() for s in lines[i].partition("$T$")]            term = lines[i + 1].lower().strip()            aspect = lines[i + 2].lower().strip()            polarity = lines[i + 3].strip()            text_raw_indices = tokenizer.text_to_sequence(text_left + " " + aspect + " " + text_right)            text_raw_without_aspect_indices = tokenizer.text_to_sequence(text_left + " " + text_right)            text_left_indices = tokenizer.text_to_sequence(text_left)            text_left_with_aspect_indices = tokenizer.text_to_sequence(text_left + " " + aspect)            text_right_indices = tokenizer.text_to_sequence(text_right, reverse=True)            text_right_with_aspect_indices = tokenizer.text_to_sequence(" " + aspect + " " + text_right, reverse=True)            aspect_indices = tokenizer.text_to_sequence(aspect)            left_context_len = np.sum(text_left_indices != 0)            aspect_len = np.sum(aspect_indices != 0)            aspect_in_text = torch.tensor([left_context_len.item(), (left_context_len + aspect_len - 1).item()])            polarity = int(polarity) + 1            text_bert_indices = tokenizer.text_to_sequence('[CLS] ' + text_left + " " + aspect + " " + text_right + ' [SEP] ' + aspect + " [SEP]")            bert_segments_ids = np.asarray([0] * (np.sum(text_raw_indices != 0) + 2) + [1] * (aspect_len + 1))            bert_segments_ids = pad_and_truncate(bert_segments_ids, tokenizer.max_seq_len)            text_raw_bert_indices = tokenizer.text_to_sequence("[CLS] " + text_left + " " + aspect + " " + text_right + " [SEP]")            aspect_bert_indices = tokenizer.text_to_sequence("[CLS] " + aspect + " [SEP]")            data = {                'text_bert_indices': text_bert_indices,                'bert_segments_ids': bert_segments_ids,                'text_raw_bert_indices': text_raw_bert_indices,                'aspect_bert_indices': aspect_bert_indices,                'text_raw_indices': text_raw_indices,                'text_raw_without_aspect_indices': text_raw_without_aspect_indices,                'text_left_indices': text_left_indices,                'text_left_with_aspect_indices': text_left_with_aspect_indices,                'text_right_indices': text_right_indices,                'text_right_with_aspect_indices': text_right_with_aspect_indices,                'aspect_indices': aspect_indices,                'aspect_in_text': aspect_in_text,                'polarity': polarity,            }            all_data.append(data)        self.data = all_data    def __getitem__(self, index):        return self.data[index]    def __len__(self):        return len(self.data)