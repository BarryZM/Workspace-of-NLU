<!-- TOC -->

- [Summary of Semantic Analysis](#summary-of-semantic-analysis)
- [Target](#target)
- [Datasets && Solutions && Metrics](#datasets--solutions--metrics)
    - [word-level](#word-level)
        - [Word Sense Disambiguation](#word-sense-disambiguation)
            - [Dataset](#dataset)
            - [solution](#solution)
            - [Metric](#metric)
        - [Word semantic represent](#word-semantic-represent)
            - [Dataset](#dataset-1)
            - [Solution](#solution)
            - [Metric](#metric-1)
    - [sentence-level](#sentence-level)
        - [match/pair/inference/retrieval/rank](#matchpairinferenceretrievalrank)
            - [Dataset](#dataset-2)
            - [Solution](#solution-1)
            - [Metric](#metric-2)
        - [textual entailment(NLI)](#textual-entailmentnli)
            - [Dataset](#dataset-3)
        - [Semantic Role Labeling](#semantic-role-labeling)
            - [Dataset](#dataset-4)
    - [document-level](#document-level)
- [Reference](#reference)
    - [Links](#links)
    - [Papers](#papers)

<!-- /TOC -->

# Summary of Semantic Analysis

# Target
+ collect semantic analysis algorithms

# Datasets && Solutions && Metrics

## word-level

### Word Sense Disambiguation

#### Dataset
+ pass
#### solution
+ Latent Semantic Indexing(隐含语义索引)
+ Latent Semantic Analysis(隐含语义分析)

#### Metric
+ pass

### Word semantic represent

#### Dataset 

| Embedding                                                    | SOTA                                                         | Tips |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ---- |
| [Project Gutenberg](https://www.gutenberg.org/)              |                                                              |      |
| [Brown University Standard Corpus of Present-Day American English](https://en.wikipedia.org/wiki/Brown_Corpus) |            |      |
| [Google 1 Billion Word Corpus](https://github.com/ciprian-chelba/1-billion-word-language-modeling-benchmark) |         |      |
| [Microsoft Research entence Completion Challenge dataset]()  | A fast and simple algorithm for training neural probabilistic language models |      |

#### Solution 

| Model                                                        | Tips                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| One-Hot                                                      | 一个数字代表一个字，one-hot 成向量                           |
| Word2Vec [paper](https://arxiv.org/pdf/1310.4546.pdf)        | CBOW;Skip-gram;Negative-sampling;Hierachical softmax         |
| Glove [paper](https://nlp.stanford.edu/pubs/glove.pdf)[website](https://nlp.stanford.edu/projects/glove/) | 词-词 共现矩阵进行分解                                       |
| Tencent [paper](https://aclweb.org/anthology/N18-2028)[website](https://aclweb.org/anthology/N18-2028[]) | 支持中文; Directional Skip-Gram                              |
| Cove                                                         |                                                              |
| ELMo [paper](https://allennlp.org/elmo)[source-code](https://github.com/allenai/bilm-tf) | Multi-layer Bi-LM                                            |
| GPT2 [blog](https://openai.com/blog/better-language-models/) [code](https://github.com/openai/gpt-2)[paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) | 使用单向 Transformer Decoder 代替 LSTM; 无监督预训练，有监督微调 |
| BERT [paper](https://arxiv.org/abs/1810.04805)[code](https://github.com/google-research/bert) | 支持中文; 双向 Encoder; Masked LM; Next sentence predict     |
| MASS [paper](https://arxiv.org/pdf/1905.02450.pdf)[code](https://github.com/microsoft/MASS) | Mask Length K;                                               |
| UniLM [paper](https://arxiv.org/pdf/1905.03197.pdf)[code]()  | Multi-task learning; NLG                                     |
| XLM [paper](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1812.10464) |                                                              |
| XLNET [paper](https://arxiv.org/abs/1906.08237)              | Transformer-XL                                               |
| ERINE [paper]()[code](https://link.zhihu.com/?target=https%3A//github.com/PaddlePaddle/LARK/tree/develop/ERNIE) | 支持中文；将BERT 中的一个字改成一个中文词                    |
| BERT-www [paper]()[code](https://github.com/ymcui/Chinese-BERT-wwm) | 支持中文；与BERT base 结构一致，使用更大量的中文预料训练     |
| MT-DNN                                                       |                                                              |

#### Metric 

+ Intrinsic Evaluation
  - Word relatedness
    - Spearman correlation (⍴) between human-labeled scores and scores generated by the embeddings on Chinese word similarity datasets wordsim-240 and wordsim-296 (translations of English resources).
  - Word Analogy
    - Accuracy on the word analogy task (e.g: “ 男人 (man) : 女 人 (woman) :: 父亲 (father) : X ”, where X chosen by cosine similarity). Different types of word analogy tasks (1) Capitals of countries (2) States/provinces of cities (3) Family words
+  Extrinsic Evaluation
  - Accuracy on Chinese sentiment analysis task
  - F1 score on Chinese named entity recognition task
  - Accuracy on part-of-speech tagging task

+ Reference
  + https://zhuanlan.zhihu.com/p/69290203

## sentence-level

### match/pair/inference/retrieval/rank
#### Dataset
| Similarity |                                                              |      |
| ---------- | ------------------------------------------------------------ | ---- |
| **LCQMC**  | LCQMC 是哈尔滨工业大学在自然语言处理国际顶会 COLING2018 构建的问题语义匹配数据集，其目标是判断两个问题的语义是否相同 |      |
|            |                                                              |      |
#### Solution

| Short Text Similarity Methods                                | Tips |
| ------------------------------------------------------------ | ---- |
| 最长公共子序列                                               |      |
| 编辑距离                                                     |      |
| 相同单词个数/序列长度                                        |      |
| word2vec+余弦相似度                                          |      |
| Sentence2Vector link                                         |      |
| DSSM(deep structured semantic models)(BOW/CNN/RNN) [link](https://www.cnblogs.com/qniguoym/p/7772561.html) |      |
| lstm+topic [link](https://blog.csdn.net/qjzcy/article/details/52269382) |      |
| Deep-siamese-text-similarity [paper](https://www.aclweb.org/anthology/W16-16#page=162)[code](https://github.com/dhwajraj/deep-siamese-text-similarity) |      |
| 聚类                                                         |      |

#### Metric
+ MAP，nDCG@20, P@20

### textual entailment(NLI)

#### Dataset

| Natural Language Inference Dataset | SOTA | Tips               |
| ---------------------------------- | ---- | ------------------ |
| XNLI                               |      | EMNLP2018:**FAIR** |
|                                    |      |                    |

### Semantic Role Labeling

#### Dataset 

| Semantic Role Labeling   | SOTA | Tips |
| ------------------------ | ---- | ---- |
| Chinese Proposition Bank |      | 中文 |
| FrameNet                 |      | 英文 |
| PropBank                 |      | 英文 |

## document-level

+ Pass


# Reference

## Links
+ [Semantic role labeling](https://en.wikipedia.org/wiki/Semantic_role_labeling)
- Semantic Parsing
  - CCG
  - DCS
  - SMT

+ 象语义表示中使用基于转移的方法学习词到概念的映射

## Papers
+ Semantic Parsing 调研综述
  - 论文搜集列表
    - https://blog.csdn.net/u013011114/article/details/79703924
  - data
    - GEO,JOBs,WebQuestions,WebQuestionsSP,WIKITABLEQUESTIONS,OVERNIGHT

+  Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base
  + 描述了基于知识库来开发问答语义解析系统的框架框架。作者说他们的方法早期使用知识库来修剪搜索空间，从而简化了语义匹配问题
  + 他们还应用高级实体链接系统和一个用于匹配问题和预测序列的深卷积神经网络模型。该模型在WebQuestions数据集上进行了测试，其性能优于以前的方法。

