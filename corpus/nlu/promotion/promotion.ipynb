{"cells":[{"cell_type":"markdown","metadata":{},"outputs":[],"source":["# 定义 intent 和 slot"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"intent = ['看不懂的','闲聊','商品/品类','搜优惠','搜活动/会场','其他']\nslot = ['日期','商品名','品牌','店铺','颜色','价格','数量','属性']\nslot_dict = {'日期':'DATE','商品/品类':'PRODUCT','品牌':'BRAND','店铺':'SHOP','颜色':'COLOR','价格':'PRICE','数量':'AMOUT','属性':'ATTRIBUTE'}"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"#dict_1 = {'B-DATE': 0, 'B-PRODUCT': 1, 'B-BRAND': 2, 'B-SHOP': 3, 'B-COLOR': 4, 'B-PRICE': 5, 'B-AMOUT': 6, 'B-ATTRIBUTE': 7, 'I-DATE': 8, 'I-PRODUCT': 9, 'I-BRAND': 10, 'I-SHOP': 11, 'I-COLOR': 12, 'I-PRICE': 13, 'I-AMOUT': 14, 'I-ATTRIBUTE': 15, 'E-DATE': 16, 'E-PRODUCT': 17, 'E-BRAND': 18, 'E-SHOP': 19, 'E-COLOR': 20, 'E-PRICE': 21, 'E-AMOUT': 22, 'E-ATTRIBUTE': 23, 'O-DATE': 24, 'O-PRODUCT': 25, 'O-BRAND': 26, 'O-SHOP': 27, 'O-COLOR': 28, 'O-PRICE': 29, 'O-AMOUT': 30, 'O-ATTRIBUTE': 31, 'S-DATE': 32, 'S-PRODUCT': 33, 'S-BRAND': 34, 'S-SHOP': 35, 'S-COLOR': 36, 'S-PRICE': 37, 'S-AMOUT': 38, 'S-ATTRIBUTE': 39, '<PAD>': 40, 'O': 41}  \ndict_1 = {'B-DATE': 0, 'B-PRODUCT': 1, 'B-BRAND': 2, 'B-SHOP': 3, 'B-COLOR': 4, 'B-PRICE': 5, 'B-AMOUT': 6, 'B-ATTRIBUTE': 7, 'I-DATE': 8, 'I-PRODUCT': 9, 'I-BRAND': 10, 'I-SHOP': 11, 'I-COLOR': 12, 'I-PRICE': 13, 'I-AMOUT': 14, 'I-ATTRIBUTE': 15, 'E-DATE': 16, 'E-PRODUCT': 17, 'E-BRAND': 18, 'E-SHOP': 19, 'E-COLOR': 20, 'E-PRICE': 21, 'E-AMOUT': 22, 'E-ATTRIBUTE': 23, 'S-DATE': 24, 'S-PRODUCT': 25, 'S-BRAND': 26, 'S-SHOP': 27, 'S-COLOR': 28, 'S-PRICE': 29, 'S-AMOUT': 30, 'S-ATTRIBUTE': 31, '<PAD>': 32, 'O': 33}\n{0: 'B-DATE', 1: 'B-PRODUCT', 2: 'B-BRAND', 3: 'B-SHOP', 4: 'B-COLOR', 5: 'B-PRICE', 6: 'B-AMOUT', 7: 'B-ATTRIBUTE', 8: 'I-DATE', 9: 'I-PRODUCT', 10: 'I-BRAND', 11: 'I-SHOP', 12: 'I-COLOR', 13: 'I-PRICE', 14: 'I-AMOUT', 15: 'I-ATTRIBUTE', 16: 'E-DATE', 17: 'E-PRODUCT', 18: 'E-BRAND', 19: 'E-SHOP', 20: 'E-COLOR', 21: 'E-PRICE', 22: 'E-AMOUT', 23: 'E-ATTRIBUTE', 24: 'S-DATE', 25: 'S-PRODUCT', 26: 'S-BRAND', 27: 'S-SHOP', 28: 'S-COLOR', 29: 'S-PRICE', 30: 'S-AMOUT', 31: 'S-ATTRIBUTE', 32: '<PAD>', 33: 'O'}\nprint(dict_1)\nprint(type(dict_1))\nfor key, value in dict_1.items():\n    print(key+'\\t'+str(value))"},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":"import pandas \nimport os\ndef read_data_by_folder(folder):\n    text_list = []\n    slot_list = []\n    \n    file_list = os.listdir(folder)\n    for file_item in file_list:\n        current_file_item = os.path.join(folder, file_item)\n        data = pandas.read_csv(current_file_item)\n        for index in data.index:\n            tmp = data.loc[index].values\n            text_list.append(tmp[0])\n\n            tmp_slot_list = []\n            for idx, value in enumerate(tmp):\n                if idx>2 and idx %2 == 1 and idx < len(tmp)-2:\n                    slot_type = tmp[idx]\n                    slot_value = tmp[idx+1]\n                    if len(str(slot_type).strip()) > 1:\n                        try:\n                            tmp_slot_list.append((slot_dict[slot_type], slot_value))\n                        except:\n                            tmp_slot_list.append(('ATTRIBUTE', slot_value))\n            slot_list.append(tmp_slot_list)\n    return text_list, slot_list\n\ndef read_clf_data_by_folder(folder):\n    clf_list = []\n    file_list = os.listdir(folder)\n    for file_item in file_list:\n        current_file_item = os.path.join(folder, file_item)\n        data = pandas.read_csv(current_file_item)\n        for index in data.index:\n            tmp = data.loc[index].values\n            clf_list.append(tmp[1]+ '\\t' +tmp[0])\n       \n    return clf_list"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["# 处理slot"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":"# 读 数据\ntext_list, slot_list = read_data_by_folder(\"/Users/sunhongchao/Desktop/Projects/1111/dataset/predict/\")\n# 写 数据\noutput = open('label.txt', encoding='utf-8', mode='w')\n\nfor text, tmp_slot_list in zip(text_list, slot_list):\n    text_label = ['O']*len(text)\n    origin_text_label = text_label.copy()\n    for slot in tmp_slot_list:\n        idx = text.index(slot[1])\n        if idx < 0:\n            continue\n        slot_type = slot[0]\n        \n        if len(slot[1]) == 1:\n            text_label[idx] = \"S\" + \"-\" + slot_type\n        else:\n            text_label[idx] = \"B\" + \"-\" + slot_type\n        \n            for idx in range(idx+1, idx+len(slot[1])-1, 1):\n                text_label[idx] = \"I\" + \"-\" + slot_type\n            \n            text_label[idx+1] = \"E\" + \"-\" + slot_type\n    \n    import operator\n    if \boperator.eq(text_label, origin_text_label) is True:\n        continue\n\n    for char, label in zip(text, text_label):\n        output.write(char + '\\t' + label + '\\n')\n    output.write('\\n')\n    \noutput.close()"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["# 处理意图"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":"import pandas \nfrom sklearn.model_selection import train_test_split\nimport random\n\nclf_list = read_clf_data_by_folder(\"/Users/sunhongchao/Desktop/Projects/1111/dataset/all_data/\")\noutput_list = []\n\nfor item in clf_list:\n    cut_list = item.split(\"\\t\")\n    if len(cut_list) != 2:\n        continue\n    if cut_list[0] in [\"其它属性\", \"看不懂的\"]:\n        continue \n    output_list.append(item)\n\nclf_list = output_list\n\nrandom.shuffle(clf_list)\n\ncut_idx = int(len(clf_list) * 0.9)\n\ntrain_clf_list = clf_list[:cut_idx]\ntest_clf_list = clf_list[cut_idx:]\n\nclf_output = open('train_clf.txt', encoding='utf-8', mode='w')\nfor item in train_clf_list:\n    clf_output.write(item + '\\n')\n\nclf_output = open('test_clf.txt', encoding='utf-8', mode='w')\nfor item in test_clf_list:\n    clf_output.write(item + '\\n')"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# 数据扩充与优化方法间的关系"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# Data augement for Unbalance NLU"},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":"import pandas \nfrom sklearn.model_selection import train_test_split\nimport random\n\nclf_list = \\\nread_clf_data_by_folder(\"/Users/sunhongchao/Desktop/Projects/1111/dataset/all_data/\")\n\nclf_output = open('clf_promotion_activity.txt', encoding='utf-8', mode='w')\nfor item in clf_list:\n    cut_list = item.split(\"\\t\")\n    if len(cut_list) != 2:\n        continue\n    if cut_list[0] not in [\"搜优惠\", \"搜活动/会场\"]:\n        continue \n    clf_output.write( item + '\\n')\n"},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":"import synonyms\nsimilarity_result = synonyms.nearby(\"明天\")\nprint(similarity_result[0])\nfrom snownlp import SnowNLP \nfor item in similarity_result[0]:                                                   \n    snow_result = SnowNLP(item)                                     \n    print(' '.join('%s/%s' % (word, tag) for (word, tag) in snow_result.tags))\n\nsimilarity_result = synonyms.nearby(\"优惠\")\nprint(similarity_result[0])\nfrom snownlp import SnowNLP \nfor item in similarity_result[0]:                                                   \n    snow_result = SnowNLP(item)                                     \n    print(' '.join('%s/%s' % (word, tag) for (word, tag) in snow_result.tags))"},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"Building prefix dict from /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/synonyms/data/vocab.txt ...\nWARNING: Logging before flag parsing goes to stderr.\nI1014 15:19:05.833382 140734836594112 __init__.py:111] Building prefix dict from /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/synonyms/data/vocab.txt ...\nLoading model from cache /var/folders/ph/31crq1pj14n9pcl2xffj45j8t0c5l2/T/jieba.u7c401a39ead7db4effe144914fc6afb3.cache\nI1014 15:19:05.841485 140734836594112 __init__.py:131] Loading model from cache /var/folders/ph/31crq1pj14n9pcl2xffj45j8t0c5l2/T/jieba.u7c401a39ead7db4effe144914fc6afb3.cache\n>> Synonyms load wordseg dict [/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/synonyms/data/vocab.txt] ... \nLoading model cost 1.731 seconds.\nI1014 15:19:07.566458 140734836594112 __init__.py:163] Loading model cost 1.731 seconds.\nPrefix dict has been built succesfully.\nI1014 15:19:07.567465 140734836594112 __init__.py:164] Prefix dict has been built succesfully.\n>> Synonyms on loading stopwords [/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/synonyms/data/stopwords.txt] ...\n>> Synonyms on loading vectors [/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/synonyms/data/words.vector] ...\n['明日', '昨天', '今晚']\n"}],"source":["def fun_similarity_words(word):\n","\n","    import synonyms\n","    similarity_result = synonyms.nearby(word)\n","    from snownlp import SnowNLP \n","    words_list = []\n","    pos_list = []\n","    return_words_list = []\n","    for item in similarity_result[0]:                                                   \n","        snow_result = SnowNLP(item)\n","        if len(words_list) is 0:\n","            words_list.append(' '.join('%s' % word for (word, tag) in snow_result.tags))\n","            pos_list.append(' '.join('/%s' % tag for (word, tag) in snow_result.tags))\n","        else:\n","            word = ' '.join('%s' % word for (word, tag) in snow_result.tags)\n","            pos =  ' '.join('/%s' % tag for (word, tag) in snow_result.tags)\n","\n","            if pos == pos_list[0]:\n","                return_words_list.append(word)\n","\n","\n","    return return_words_list\n","\n","print(fun_similarity_words(\"明天\"))\n","\n",""]},{"cell_type":"markdown","execution_count":18,"metadata":{},"outputs":[],"source":"' '.join('%s/%s' % (word, tag) for (word, tag) in snow_result.tags))\n\nsimilarity_result = synonyms.nearby(\"优惠\")\nprint(similarity_result[0])\n"},{"cell_type":"markdown","execution_count":43,"metadata":{},"outputs":[],"source":"# 数据生成"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"from snownlp import SnowNLP \n\noutput_file = open('corpus/nlu/promotion/clf_augement.txt', mode='w', encoding='utf-8')\n \nwith open('corpus/nlu/promotion/clf_promotion_activity.txt', mode='r', encoding='utf-8') as f:\n    lines = f.readlines()\n    for line in lines:\n        # print(\"*\" *50)\n        # print(line)\n        snow_result = SnowNLP(\"\".join(line.split(\"\\t\")[1:])) # 读取原始文本\n        origin_tag = line.split(\"\\t\")[0]\n        # print(\"before generation\")                             \n        # print(' '.join('%s/%s' % (word, tag) for (word, tag) in snow_result.tags))\n\n        # print(\"\\nafter generation\")\n        words = ' '.join('%s' % word for (word, tag) in snow_result.tags)\n        word_list = words.split()\n\n        for idx, word in enumerate(word_list):\n            sim_word_list = fun_similarity_words(word)\n            if len(sim_word_list) is 0:\n                continue\n             \n            # print(word + \": similarity words list :\", sim_word_list)\n\n            # print(\"\\n output results\")\n            for sim_word in sim_word_list:\n                if idx != len(word_list)-1:\n                    tmp_str=\"\".join(word_list[:idx]) + sim_word +  \"\".join(word_list[idx+1:])\n                    # print(tmp_str)\n                    output_file.write(origin_tag + '\\t' + tmp_str + '\\n')\n\n                else:\n                    tmp_str = \"\".join(word_list[:idx]) + sim_word\n                    # print(tmp_str)\n                    output_file.write(origin_tag + '\\t' + tmp_str + '\\n')\n                        \noutput_file.close()"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}